{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fa42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import CocoFlir\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acd39605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(img_path):\n",
    "    \"\"\"Keep reading image until succeed.\n",
    "    This can avoid IOError incurred by heavy IO process.\"\"\"\n",
    "    got_img = False\n",
    "    print(img_path)\n",
    "    if not osp.exists(img_path):\n",
    "        raise IOError(\"{} does not exist\".format(img_path))\n",
    "    while not got_img:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            got_img = True\n",
    "        except IOError:\n",
    "            print(\"IOError incurred when reading '{}'. Will redo. Don't worry. Just chill.\".format(img_path))\n",
    "            pass\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0c6c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os.path as osp\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, pid, camid,trackid, idx = self.dataset[index]\n",
    "        if isinstance(img_path,tuple):\n",
    "            all_imgs = []\n",
    "            all_imgs_path = []\n",
    "            for i_path in img_path:\n",
    "                i_img = read_image(i_path)\n",
    "                if self.transform is not None:\n",
    "                    i_img = self.transform(i_img)\n",
    "                all_imgs.append(i_img)\n",
    "                all_imgs_path.append(i_path)\n",
    "                # all_imgs_path.append(i_path.split('/')[-1])\n",
    "            img = tuple(all_imgs)\n",
    "\n",
    "            # print('data base pid ',pid)\n",
    "            if isinstance(pid, tuple):\n",
    "                if isinstance(idx, tuple):\n",
    "                    return img + pid + (camid, trackid)+ tuple(all_imgs_path)+ idx\n",
    "                else:\n",
    "                    return img + pid + (camid, trackid, tuple(all_imgs_path), idx)\n",
    "            else:\n",
    "                return img + (pid, camid, trackid, tuple(all_imgs_path), idx)\n",
    "        else:\n",
    "            img = read_image(img_path)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "           \n",
    "            return img, pid, camid, trackid, img_path.split('/')[-1],idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa0a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(batch):\n",
    "\n",
    "    imgs, pids, camids, viewids , _ , idx= zip(*batch)\n",
    "    # print('train collate fn' , imgs)\n",
    "    pids = torch.tensor(pids, dtype=torch.int64)\n",
    "    viewids = torch.tensor(viewids, dtype=torch.int64)\n",
    "    camids = torch.tensor(camids, dtype=torch.int64)\n",
    "    return torch.stack(imgs, dim=0), pids, camids, viewids, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4cfb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader():\n",
    "    \n",
    "    train_transforms = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.RandomCrop((224, 224)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transforms = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "#     num_workers = cfg.DATALOADER.NUM_WORKERS\n",
    "    dataset = CocoFlir(root_train='/home/rufael.marew/Documents/Academics/AI702/project/test/sgada_data/mscoco/mscoco.txt',\\\n",
    "                       root_val='/home/rufael.marew/Documents/Academics/AI702/project/test/sgada_data/flir/flir.txt'\n",
    "                       , plus_num_id=100)\n",
    "    train_set = ImageDataset(dataset.train, train_transforms)\n",
    "    train_set1 = ImageDataset(dataset.train, val_transforms)\n",
    "    \n",
    "    train_set_normal = ImageDataset(dataset.train, val_transforms)\n",
    "    img_num1 = len(dataset.train)\n",
    "    \n",
    "\n",
    "   \n",
    "    num_classes = max(dataset.num_train_pids, dataset.num_test_pids)\n",
    "    cam_num = dataset.num_train_cams\n",
    "    view_num = dataset.num_train_vids\n",
    "    \n",
    "    print('use shuffle sampler strategy')\n",
    "    train_loader = DataLoader(\n",
    "    train_set, batch_size=32, shuffle=True, num_workers=2,\n",
    "    collate_fn=train_collate_fn\n",
    "    )\n",
    "        \n",
    "   \n",
    "            \n",
    "#     val_loader = DataLoader(\n",
    "#         val_set, batch_size=32, shuffle=False, num_workers=1,\n",
    "#         collate_fn=val_collate_fn\n",
    "#     )\n",
    "#     train_loader_normal = DataLoader(\n",
    "#         train_set_normal, batch_size=cfg.TEST.IMS_PER_BATCH, shuffle=False, num_workers=num_workers,\n",
    "#         collate_fn=val_collate_fn\n",
    "#     )\n",
    "    \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae50dbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Coco-Flir loaded\n",
      "Dataset statistics:\n",
      "train mscoco and valid is flir\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train   |     3 |   253347 |         1\n",
      "  valid   |     3 |    74168 |         1\n",
      "  ----------------------------------------\n",
      "use shuffle sampler strategy\n"
     ]
    }
   ],
   "source": [
    "train_loader = make_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "787c10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "import os\n",
    "class Coco(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, label_path, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.label = pd.read_csv(label_path, delimiter=' ')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.label.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.label.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b915906",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "#         T.RandomCrop((224, 224)),\n",
    "#         T.RandomHorizontalFlip(),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "training_data = Coco(label_path='/home/rufael.marew/Documents/Academics/AI702/project/test/sgada_data/mscoco/mscoco.txt',\n",
    "                    root_dir='/home/rufael.marew/Documents/Academics/AI702/project/test/sgada_data/',transform=train_transforms)\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b724ae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([1, 3, 256, 256])\n",
      "Labels batch shape: torch.Size([1])\n",
      "torch.Size([3, 256, 256])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "print(img.shape)\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9ad03d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-cd629bdbf9fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Concatenate the images into a single numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Use t-SNE to reduce the dimensionality of the images to 2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from sklearn.manifold import TSNE\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Assuming you have a PyTorch DataLoader object named `data_loader`\n",
    "# that contains your dataset:\n",
    "\n",
    "# Define a transform to convert the images to numpy arrays\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Collect all the images in the data into a list\n",
    "images = []\n",
    "for batch in train_dataloader:\n",
    "    for image in batch:\n",
    "        # Convert each image to a numpy array and append to the list\n",
    "        images.append(image.numpy())\n",
    "\n",
    "# Concatenate the images into a single numpy array\n",
    "images = np.concatenate(images)\n",
    "\n",
    "# Use t-SNE to reduce the dimensionality of the images to 2D\n",
    "tsne = TSNE(n_components=2)\n",
    "images_tsne = tsne.fit_transform(images)\n",
    "\n",
    "# Plot a scatter plot of the t-SNE coordinates\n",
    "plt.scatter(images_tsne[:,0], images_tsne[:,1])\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8dfe6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        images = []\n",
    "        labels = []\n",
    "        self.root_dir = root_dir\n",
    "        for i in os.listdir(root_dir):\n",
    "            images.append(os.listdir(os.path.join(root_dir,i)))\n",
    "            labels.append(i)\n",
    "        self.image_files = images\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Load images and convert them to tensors\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for img_path,label in zip(self.image_files,labels):\n",
    "#             print(img_path)\n",
    "            for image_file in img_path:\n",
    "                image_path = os.path.join(os.path.join(self.root_dir, label), image_file)\n",
    "                \n",
    "                image = self.transform(Image.open(image_path))\n",
    "                self.images.append(image)\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        # Perform PCA on the image tensors to reduce dimensionality\n",
    "        image_vectors = torch.stack(self.images).numpy().reshape(3,-1)\n",
    "        df = pd.DataFrame(image_vectors)\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_vectors = pca.fit_transform(image_vectors)\n",
    "        self.pca_vectors = torch.from_numpy(pca_vectors)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx]\n",
    "    \n",
    "    def visualize(self):\n",
    "        unique_labels = list(set(self.labels))\n",
    "        label_colors = {label: i for i, label in enumerate(unique_labels)}\n",
    "        colors = [label_colors[label] for label in self.labels]\n",
    "        plt.scatter(self.pca_vectors[:, 0], self.pca_vectors[:, 1], c=colors, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7995579",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4860674048 into shape (3,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-79ce5b6249c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dataset = ImageDataset('/home/rufael.marew/Documents/Academics/AI702/project/CDTrans/data/cocoflir/flir')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/rufael.marew/Documents/Academics/AI702/project/test/sgada_data/flir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-25b2713315cb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Perform PCA on the image tensors to reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mimage_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 4860674048 into shape (3,newaxis)"
     ]
    }
   ],
   "source": [
    "# dataset = ImageDataset('/home/rufael.marew/Documents/Academics/AI702/project/CDTrans/data/cocoflir/flir')\n",
    "dataset = ImageDataset('/home/rufael.marew/Documents/Academics/AI702/project/test/sgada_data/flir')\n",
    "dataset.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f607076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
